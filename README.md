# Sanskrit RAG â€” CPU-Only Retrieval-Augmented Generation

## Overview

This project implements an **end-to-end Retrieval-Augmented Generation (RAG) system for Sanskrit documents**, running entirely on **CPU (no GPU required)**.

The system:

- Ingests Sanskrit documents (`.pdf` / `.txt`)
- Retrieves relevant context using **vector search**
- Generates **grounded answers** using a **local LLM**
- Works **offline after one-time model download**

---

## Why CPU-Only?

- GPU usage is **not permitted by the assignment**
- Ensures **portability and local execution**
- Uses **CPU-friendly open-source models**
- **No external APIs** (OpenAI, cloud services) required

CPU-only execution is enforced by:

- FAISS **CPU** index
- Hugging Face models loaded with **CPU settings**
- No CUDA or `device_map` usage

---

## ðŸŽ¥ Project Videos

### ðŸ“Œ Video Pitch
A short overview explaining the motivation, architecture, and key features of the CPU-only Sanskrit RAG system.

ðŸ‘‰ Watch here: https://drive.google.com/file/d/1XibWBzIFEEARokP5ZmOnD62tpRx0epmv/view?usp=sharing

---

### ðŸ§ª Demonstration Video
A step-by-step demonstration of document ingestion, retrieval, and answer generation using the system.

ðŸ‘‰ Watch here: https://drive.google.com/your-demo-video-link

---

## Repository Structure

```
RAG_Sanskrit_Yogesh/
â”‚
â”œâ”€â”€ code/
â”‚ â”œâ”€â”€ main.py         # Entry point (run this)
â”‚ â”‚
â”‚ â”œâ”€â”€ config.py       # Model names, paths, parameters
â”‚ â”‚
â”‚ â”œâ”€â”€ data_loader.py  # Load .txt / .pdf Sanskrit documents
â”‚ â”‚
â”‚ â”œâ”€â”€ preprocessor.py # Sanskrit text cleaning & normalization
â”‚ â”‚
â”‚ â”œâ”€â”€ chunker.py      # Text chunking logic
â”‚ â”‚
â”‚ â”œâ”€â”€ embeddings.py   # Load embedding model (CPU, local)
â”‚ â”‚
â”‚ â”œâ”€â”€ indexer.py      # Create & save FAISS index
â”‚ â”‚
â”‚ â”œâ”€â”€ retriever.py    # Retrieve top-k relevant chunks
â”‚ â”‚
â”‚ â”œâ”€â”€ generator.py    # CPU-based LLM response generation
â”‚ â”‚
â”‚ â””â”€â”€ rag_pipeline.py # Connect retriever + generator
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â””â”€â”€ rag-docs.pdf
â”‚ â”‚
â”‚ â””â”€â”€ processed/
â”‚ â”œâ”€â”€ cleaned_text.txt
â”‚ â””â”€â”€ chunks.json
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ embeddings/
â”‚ â”‚ â””â”€â”€ paraphrase-multilingual-MiniLM-L12-v2/
â”‚ â”‚
â”‚ â””â”€â”€ llm/
â”‚ â””â”€â”€ flan-t5-base/
â”‚
â”œâ”€â”€ vectorstore/
â”‚ â”œâ”€â”€ faiss.index # FAISS CPU index
â”‚ â””â”€â”€ metadata.pkl # Chunk metadata
â”‚
â”œâ”€â”€ report/
â”‚ â””â”€â”€ Sanskrit_RAG_Report.pdf
â”œâ”€â”€ images/
â”‚ â””â”€â”€ System_Architecture.png
â”‚
â”œâ”€â”€ demo/
â”‚ â””â”€â”€ demo_video.mp4 # Optional demo video
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
## System Architecture
![RAG Sanskrit System Architecture](https://github.com/user-attachments/assets/ad1abdc9-d269-40f3-96ec-a22930d1582d)


---

## Requirements

- Python **3.9+**
- Windows / Linux / macOS
- Internet connection (**first run only**, for model download)
- Virtual environment is **optional**
- Project can be run using **system Python**

---

## Install Dependencies

```bash
pip install -r requirements.txt
```

---

## Place Sanskrit Documents

Put your Sanskrit `.pdf` or `.txt` files inside, e.g.:

```
data/raw/Rag-docs.pdf
```

---

## One-Time Model Download (IMPORTANT)

Models are downloaded **once** and cached locally. After this step, the system works **completely offline**.

---

### Download Embedding Model

**Model:**

```
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
```

Create a temporary file in the project root: `download_embeddings.py`

```python
from sentence_transformers import SentenceTransformer

SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    cache_dir="models/embeddings"
)
```

Run:

```bash
python download_embeddings.py
```

You may delete the file after download.

### Download Generator Model (LLM)

**Model:** `google/flan-t5-base`

Create a temporary file: `download_llm.py`

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

AutoTokenizer.from_pretrained(
    "google/flan-t5-base",
    cache_dir="models/llm"
)
AutoModelForSeq2SeqLM.from_pretrained(
    "google/flan-t5-base",
    cache_dir="models/llm"
)
```

Run:

```bash
python download_llm.py
```

---

## Build the Index

This step performs:

- PDF text extraction
- Text cleaning and normalization
- Chunking
- Embedding generation
- FAISS index construction

Run:

```bash
python code/main.py --build
```

Generated artifacts:

- `vectorstore/faiss.index`
- `vectorstore/metadata.pkl`
- `data/processed/cleaned_text.txt`
- `data/processed/chunks.json`

---

## Query the System

### Example Queries

```bash
python code/main.py --query "Who is Kalidasa?"
python code/main.py --query "What lesson is taught in the story of the devotee?"
python code/main.py --query "à¤•à¤¾à¤²à¥€à¤¦à¤¾à¤¸à¤ƒ à¤•à¤ƒ?"
```

---

## Configuration

All configurable parameters are defined in:

```
code/config.py
```

You can modify:

- Embedding model
- Generator (LLM) model
- Chunk size and overlap
- Top-K retrieval count

---

## Known Limitations

- Sanskrit text may be OCR-corrupted
- Limited document corpus
- CPU-based LLM inference is slow
- Abstract reasoning capability is limited

These constraints are expected and documented.

---

## Troubleshooting

**Answer could not be generated**

- Use entity-based questions
- Prefer simple Sanskrit or English
- Ensure index build completed successfully

**Slow performance**

- First run includes model downloads
- CPU inference is slower than GPU
- Reduce `TOP_K` in `code/config.py`

---

## Future Improvements

- Improved Sanskrit OCR cleanup
- Roman-to-Devanagari transliteration
- Quantized CPU LLMs (GGUF / GPTQ)
- Larger Sanskrit document corpus
- Retrieval quality evaluation metrics

---

## Conclusion

This project demonstrates a fully offline, CPU-only Retrieval-Augmented Generation system for Sanskrit documents, following standard RAG architecture and best practices. Despite OCR noise and limited data, it successfully retrieves relevant Sanskrit context and generates grounded answers.

---

## License

Open-source, for academic and learning purposes.
